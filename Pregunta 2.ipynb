{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 Máquinas de Aprendizaje II-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 3: Pregunta 2 </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "* Andrés Huerta - 201473544-8 - andres.huerta.14@sansano.usm.cl\n",
    "* Felipe Vega - 201473511-1 - felipe.vega.14@sansano.usm.cl\n",
    "\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Imports Here :D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\"/>\n",
    "# Redes Convolucionales sobre imágenes\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **a) Construya funciones para leer los datos y cargarlos al momento de entrenar (durante cada *epoch*), para ésto utilice *Image Data Generator* de keras.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de querer ejecutar el cuaderno, para acelerar la ejecución se guardaron los modelos ya entrenados en archivos aparte.\n",
    "\n",
    "<font size=5> [Link a modelos ya entrenados](https://drive.google.com/file/d/10KWWkXUkebwefi8ZMssGPaK3uWIfIwQm/view?usp=sharing) </font>\n",
    "\n",
    "Al descargar, por favor descomprimir en la misma carpeta que este cuaderno.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 3 classes.\n",
      "Found 600 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #no transformation\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'food_data/train',\n",
    "        target_size=(150, 150),\n",
    "        color_mode='rgb',\n",
    "        batch_size=32)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'food_data/val',\n",
    "        target_size=(150, 150),\n",
    "        color_mode='rgb',\n",
    "        batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan los sets de entrenamiento y testing, para el set de entrenamiento se cuenta con un total de 2400 imagenes repartidas equitativamente entre hotdogs, hamburguesas y pizzas. El test de testing a su vez cuenta con 600 imagenes donde cada clase tiene un total de 200 imagenes.\n",
    "\n",
    "Analizando algunas de las imagenes individualmente en la carpeta contenedora se nota que existen algunas imagenes que no muestran los elementos principales o en algunas ocasiones estas aparecen muy pequeñas, dichas imagenes podrían quizas ser ruido para el modelo a entrenar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **b) Utilice la red tradicional (*Feed Forward*) entregada en el código para ser entrenada sobre los datos vectorizados, esto es que cada imagen queda representada como un vector gigante, y las 3 clases a las que se enfrenta. Evalúe el modelo con la métrica *accuracy* sobre el conjunto de validación.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_FFN():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_generator.image_shape)) #full dense\n",
    "    model.add(BatchNormalization()) #to normalize the input..\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(128,activation='relu')) #128\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Let's train the model using RMSprop\n",
    "    model.summary()\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator.classes)//train_generator.batch_size, #samples//batch_size\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator.classes)//validation_generator.batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza una red neuronal FF para clasificación con un total de 3 capas, siendo solo 1 la capa oculta.\n",
    "\n",
    "Debido a que se esta tratando con un problema de clasificación multiple exclusivo se utiliza como función de perdida catergorical_crossentropy.\n",
    "\n",
    "ademas, para evitar el re-entrenamiento si se borra el modelo se guarda localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = load_model(\"fnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de querer entrenarlo nuevamente descomentar esta sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 67500)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 67500)             270000    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               17280256  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 17,583,539\n",
      "Trainable params: 17,448,539\n",
      "Non-trainable params: 135,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 44s 589ms/step - loss: 8.9350 - acc: 0.4225 - val_loss: 8.8570 - val_acc: 0.4444\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 42s 559ms/step - loss: 8.4477 - acc: 0.4704 - val_loss: 8.6033 - val_acc: 0.4613\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 40s 536ms/step - loss: 8.9964 - acc: 0.4354 - val_loss: 8.9693 - val_acc: 0.4419\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 42s 559ms/step - loss: 8.2257 - acc: 0.4858 - val_loss: 8.5033 - val_acc: 0.4701\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 47s 620ms/step - loss: 8.5635 - acc: 0.4667 - val_loss: 8.4848 - val_acc: 0.4683\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 48s 644ms/step - loss: 8.7432 - acc: 0.4513 - val_loss: 9.1657 - val_acc: 0.4313\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 45s 605ms/step - loss: 8.7318 - acc: 0.4554 - val_loss: 9.1470 - val_acc: 0.4296\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 46s 616ms/step - loss: 8.6916 - acc: 0.4588 - val_loss: 8.8275 - val_acc: 0.4489\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 47s 620ms/step - loss: 8.1219 - acc: 0.4937 - val_loss: 7.7678 - val_acc: 0.5176\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 52s 698ms/step - loss: 8.4780 - acc: 0.4725 - val_loss: 8.3191 - val_acc: 0.4789\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 63s 836ms/step - loss: 8.4896 - acc: 0.4708 - val_loss: 8.2109 - val_acc: 0.4877\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 58s 780ms/step - loss: 7.9750 - acc: 0.5017 - val_loss: 8.7059 - val_acc: 0.4595\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 50s 665ms/step - loss: 8.3171 - acc: 0.4804 - val_loss: 8.2685 - val_acc: 0.4859\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 50s 663ms/step - loss: 8.3283 - acc: 0.4808 - val_loss: 8.8030 - val_acc: 0.4507\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 54s 726ms/step - loss: 8.3884 - acc: 0.4771 - val_loss: 8.5137 - val_acc: 0.4718\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 56s 748ms/step - loss: 8.3870 - acc: 0.4783 - val_loss: 8.0988 - val_acc: 0.4912\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 53s 710ms/step - loss: 8.5228 - acc: 0.4688 - val_loss: 8.6469 - val_acc: 0.4630\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 59s 793ms/step - loss: 8.5378 - acc: 0.4692 - val_loss: 7.7516 - val_acc: 0.5158\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 58s 777ms/step - loss: 8.5279 - acc: 0.4700 - val_loss: 8.0411 - val_acc: 0.5000\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 58s 777ms/step - loss: 8.4519 - acc: 0.4733 - val_loss: 7.6380 - val_acc: 0.5243\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 56s 753ms/step - loss: 8.5163 - acc: 0.4704 - val_loss: 8.0399 - val_acc: 0.4982\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 58s 779ms/step - loss: 8.1896 - acc: 0.4900 - val_loss: 7.9801 - val_acc: 0.5035\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 53s 700ms/step - loss: 8.2994 - acc: 0.4838 - val_loss: 7.8642 - val_acc: 0.5106\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 57s 759ms/step - loss: 8.1893 - acc: 0.4904 - val_loss: 7.9228 - val_acc: 0.5070\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 56s 753ms/step - loss: 8.4760 - acc: 0.4721 - val_loss: 8.4031 - val_acc: 0.4771\n"
     ]
    }
   ],
   "source": [
    "#ffn = do_FFN()\n",
    "#ffn.save(\"fnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validation:  0.4849999996026357\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy validation: \",ffn.evaluate_generator(generator=validation_generator)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene un accuracy de 0.48 con el set de testing. Una manera de mejorar este score podría ser limpiar el dataset de entrenamiento, por ejemplo eliminando aquellas imagenes que no presentan la comida de manera reconocible. La ingeniería de atributos es la clave!\n",
    "\n",
    "Otra forma de mejorar el desempeño, sería sintonizar los parametros por medio de una validación cruzada, se podría ademas intentar comprobar el desempeño con otro optimizador, pero debido al alto uso de GPU esta opción podría demorar mucho tiempo (el cual lamentablemente en este periodo no disponemos :c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">** c) Utilice la red convolucional (*CNN*) entregada en el código para ser entrenada sobre los datos brutos, matrices RGB de píxeles, y las 3 clases a las que se enfrenta. Evalúe el modelo con la métrica *accuracy* sobre el conjunto de validación. Compare.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_CNN(train_generator):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',input_shape=train_generator.image_shape,activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3),activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3),activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(len(train_generator.class_indices),activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Let's train the model using RMSprop\n",
    "    model.summary()\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator.classes)//train_generator.batch_size, #samples//batch_size\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator.classes)//validation_generator.batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se genera una red de convolución, la cual consiste en separar las matrices de las imagenes en matrices de menor tamaño (dividir y conquistar) el trade-off que se obtiene de esta aplicación es que, en general, el desempeño del modelo debería aumentar, pero a cambio de un mayor costo de ejecución (cada epoch de esta parte demora un total de 5 minutos o más en completarse).\n",
    "\n",
    "El optimizador utilizado nuevamente corresponde a rmsprop, el cual de acuerdo a la documentación es efectivo cuando se esta trabajando con modelos recurrentes.\n",
    "\n",
    "La red generada posee 4 capas convolucionales las cuales subdividen el problema en zancadas de 3x3, al final de la red se agrupan todas las redes formadas y se trabaja como una red ff.\n",
    "\n",
    "Se entrena la red y nuevamente se guarda la red localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = load_model(\"red_convolucion.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 150, 150, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 148, 148, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 74, 74, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 72, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               10616960  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 10,682,915\n",
      "Trainable params: 10,682,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 353s 5s/step - loss: 1.2047 - acc: 0.3733 - val_loss: 1.0964 - val_acc: 0.4132\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 362s 5s/step - loss: 1.0697 - acc: 0.4504 - val_loss: 1.0532 - val_acc: 0.4489\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 371s 5s/step - loss: 1.0242 - acc: 0.5050 - val_loss: 0.9929 - val_acc: 0.5246\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 386s 5s/step - loss: 0.9710 - acc: 0.5454 - val_loss: 0.9359 - val_acc: 0.5581\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 398s 5s/step - loss: 0.9458 - acc: 0.5683 - val_loss: 0.9973 - val_acc: 0.4859\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 408s 5s/step - loss: 0.9146 - acc: 0.5896 - val_loss: 0.8957 - val_acc: 0.6039\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 407s 5s/step - loss: 0.8931 - acc: 0.6054 - val_loss: 0.8590 - val_acc: 0.6250\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 412s 5s/step - loss: 0.8473 - acc: 0.6213 - val_loss: 0.8595 - val_acc: 0.6092\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 424s 6s/step - loss: 0.8345 - acc: 0.6371 - val_loss: 0.8594 - val_acc: 0.6250\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 470s 6s/step - loss: 0.8172 - acc: 0.6400 - val_loss: 0.8344 - val_acc: 0.6092\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 549s 7s/step - loss: 0.7818 - acc: 0.6587 - val_loss: 0.8165 - val_acc: 0.6673\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 516s 7s/step - loss: 0.7683 - acc: 0.6704 - val_loss: 0.7854 - val_acc: 0.6496\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 460s 6s/step - loss: 0.7519 - acc: 0.6896 - val_loss: 0.7347 - val_acc: 0.7007\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 437s 6s/step - loss: 0.7324 - acc: 0.6929 - val_loss: 0.7677 - val_acc: 0.6690\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 425s 6s/step - loss: 0.7216 - acc: 0.6979 - val_loss: 0.7280 - val_acc: 0.6831\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 432s 6s/step - loss: 0.6882 - acc: 0.7071 - val_loss: 0.8074 - val_acc: 0.6180\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 443s 6s/step - loss: 0.6846 - acc: 0.7200 - val_loss: 0.7224 - val_acc: 0.7095\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 437s 6s/step - loss: 0.6661 - acc: 0.7279 - val_loss: 0.6545 - val_acc: 0.7430\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 422s 6s/step - loss: 0.6564 - acc: 0.7292 - val_loss: 0.6575 - val_acc: 0.7201\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 424s 6s/step - loss: 0.6411 - acc: 0.7317 - val_loss: 0.7076 - val_acc: 0.7101\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 417s 6s/step - loss: 0.6279 - acc: 0.7471 - val_loss: 0.6713 - val_acc: 0.7306\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 422s 6s/step - loss: 0.6331 - acc: 0.7358 - val_loss: 0.6870 - val_acc: 0.7165\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 418s 6s/step - loss: 0.6118 - acc: 0.7467 - val_loss: 0.5863 - val_acc: 0.7500\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 446s 6s/step - loss: 0.6168 - acc: 0.7546 - val_loss: 1.0339 - val_acc: 0.5722\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 418s 6s/step - loss: 0.5837 - acc: 0.7633 - val_loss: 0.7015 - val_acc: 0.6725\n"
     ]
    }
   ],
   "source": [
    "#cnn = do_CNN(train_generator)\n",
    "#cnn.save(\"red_convolucion.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza la metrica accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN:\n",
      "Accuracy validation:  0.696666665871938\n",
      "FFN:\n",
      "Accuracy validation:  0.4849999992052714\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN:\")\n",
    "print(\"Accuracy validation: \",cnn.evaluate_generator(generator=validation_generator)[1])\n",
    "\n",
    "print(\"FFN:\")\n",
    "print(\"Accuracy validation: \",ffn.evaluate_generator(generator=validation_generator)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se comento anteriormente una red de convolución tiende a mejorar el desempeño para modelos que utilicen matrices de gran tamaño, en este caso se ve una clara mejora de mas de 0.2 puntos sobre el set de testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **d) Genere un conjunto datos con incorrecta etiquetación de manera manual y vea si el modelo convolucional se sigue comportando de la misma manera. Para esto tome 100 imágenes aleatorias de entrenamiento de la carpeta *hot dog* y 100 imágenes aleatorias de entrenamiento de la carpeta *hamburger* e intercambielas, sin manipular las imágenes de la carpeta *pizza* y con el conjunto de validación intacto. Genere las matrices de confusión en el conjunto de validación para visualizar cómo afectó al modelo la corrupción realizada a los datos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se creó una nueva carpeta con 100 de las imagenes de hot dogs con 100 imagenes de hamburguesas y se genera el nuevo conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator2 = train_datagen.flow_from_directory(\n",
    "        'food_data/train2',\n",
    "        target_size=(150, 150),\n",
    "        color_mode='rgb',\n",
    "        batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se reentrena la red, pero con los datos sucios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_noise = load_model(\"convolucion_noise.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 150, 150, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 148, 148, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 74, 74, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 72, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               10616960  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 10,682,915\n",
      "Trainable params: 10,682,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 349s 5s/step - loss: 1.1954 - acc: 0.4183 - val_loss: 1.0263 - val_acc: 0.5017\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 374s 5s/step - loss: 1.0522 - acc: 0.4475 - val_loss: 0.9814 - val_acc: 0.5211\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 466s 6s/step - loss: 1.0169 - acc: 0.5021 - val_loss: 0.9458 - val_acc: 0.5898\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 461s 6s/step - loss: 0.9834 - acc: 0.5229 - val_loss: 0.9939 - val_acc: 0.5000\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 447s 6s/step - loss: 0.9423 - acc: 0.5567 - val_loss: 0.8974 - val_acc: 0.5599\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 496s 7s/step - loss: 0.9377 - acc: 0.5575 - val_loss: 0.8647 - val_acc: 0.5915\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 481s 6s/step - loss: 0.9267 - acc: 0.5729 - val_loss: 0.8571 - val_acc: 0.6004\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 445s 6s/step - loss: 0.8562 - acc: 0.6125 - val_loss: 1.2356 - val_acc: 0.4542\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 448s 6s/step - loss: 0.8651 - acc: 0.5929 - val_loss: 0.9234 - val_acc: 0.5898\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 473s 6s/step - loss: 0.8429 - acc: 0.6229 - val_loss: 0.8938 - val_acc: 0.5827\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 488s 7s/step - loss: 0.8377 - acc: 0.6083 - val_loss: 0.8639 - val_acc: 0.5933\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 496s 7s/step - loss: 0.8013 - acc: 0.6392 - val_loss: 0.7683 - val_acc: 0.6303\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 513s 7s/step - loss: 0.8200 - acc: 0.6304 - val_loss: 1.0204 - val_acc: 0.5775\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 473s 6s/step - loss: 0.7777 - acc: 0.6467 - val_loss: 0.7448 - val_acc: 0.6426\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 479s 6s/step - loss: 0.7780 - acc: 0.6433 - val_loss: 0.8040 - val_acc: 0.6320\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 523s 7s/step - loss: 0.7410 - acc: 0.6721 - val_loss: 0.7433 - val_acc: 0.6567\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 507s 7s/step - loss: 0.7597 - acc: 0.6583 - val_loss: 0.7360 - val_acc: 0.6725\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 543s 7s/step - loss: 0.7393 - acc: 0.6717 - val_loss: 0.8225 - val_acc: 0.6356\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 527s 7s/step - loss: 0.7211 - acc: 0.6863 - val_loss: 0.8506 - val_acc: 0.6303\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 462s 6s/step - loss: 0.7304 - acc: 0.6725 - val_loss: 0.6845 - val_acc: 0.6858\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 486s 6s/step - loss: 0.7165 - acc: 0.6787 - val_loss: 0.6972 - val_acc: 0.6761\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 481s 6s/step - loss: 0.6880 - acc: 0.6979 - val_loss: 0.7553 - val_acc: 0.6620\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 470s 6s/step - loss: 0.6915 - acc: 0.6917 - val_loss: 0.7270 - val_acc: 0.6532\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 486s 6s/step - loss: 0.6981 - acc: 0.6892 - val_loss: 0.7487 - val_acc: 0.6655\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 473s 6s/step - loss: 0.6685 - acc: 0.6942 - val_loss: 0.7216 - val_acc: 0.6743\n"
     ]
    }
   ],
   "source": [
    "#cnn_noise = do_CNN(train_generator2)\n",
    "#cnn_noise.save(\"convolucion_noise.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza una comparación simple del desempeño de cada uno de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN limpia:\n",
      "Accuracy validation:  0.6966666666666667\n",
      "CNN sucia:\n",
      "Accuracy validation:  0.6716666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN limpia:\")\n",
    "print(\"Accuracy validation: \",cnn.evaluate_generator(generator=validation_generator)[1])\n",
    "\n",
    "print(\"CNN sucia:\")\n",
    "print(\"Accuracy validation: \",cnn_noise.evaluate_generator(generator=validation_generator)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando una metrica de desempeño simple se puede ver que la calidad del modelo de la red de convolución sucia, disminuye su magnitud, aun asi, la diferencia es de un poco mas de 0.02 puntos lo cual es sorprendente considerando que se esta entrenando erroneamente el 12,5% de los datos de hotdogs y hamburguesas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza una matriz de confusión para ver más a fondo el comportamiento de las redes convolucionales entrenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_noise = cnn_noise.predict_generator(validation_generator,len(validation_generator.classes)//validation_generator.batch_size+1)\n",
    "y_pred_noise = np.argmax(Y_pred_noise, axis=1)\n",
    "\n",
    "Y_pred = cnn.predict_generator(validation_generator,len(validation_generator.classes)//validation_generator.batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "Y_pred_ffn = ffn.predict_generator(validation_generator,len(validation_generator.classes)//validation_generator.batch_size+1)\n",
    "y_pred_ffn = np.argmax(Y_pred_ffn, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea ademas una matriz de confusión para la red ff, solo para comparar el comportamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_m = confusion_matrix(validation_generator.classes, y_pred)\n",
    "confusion_mn = confusion_matrix(validation_generator.classes, y_pred_noise)\n",
    "confusion_mf = confusion_matrix(validation_generator.classes, y_pred_ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FF:\")\n",
    "confusion_mf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que la red feed-forward presenta problemas para clasificar la primera clase, clasificando correctamente solo el 8% de los datos de testing de dicha clase, la mejor clasificación que realiza corresponde a la tercera clase, alcanzando un poco mas del 50% de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CNN limpio: \")\n",
    "confusion_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la red de convolución con los datos limpios se puede ver una mejora considerable para la primera clase, subiendo de un 8% de prcisión a un 35%, la precisión de la segunda clase también aumento, pero la tercera clase presenta una perdida de calidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CNN sucio\")\n",
    "confusion_mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se visualiza la matriz de confusión de la red de convolución con datos sucios, y se puede notar que la gran diferencia entre los resultados esta presente en la distribución de las imagenes clasificadas incorrectamente, a pesar de que el desempeño para las correctas clasificaciones disminuye se puede notar que no es una diferencia muy grande, lo cual resulta bastante interesante.\n",
    "\n",
    "Analizando el desempeño de la tercera posibilidad de clasificación se puede asumir que dicha clase corresponde a las pizzas, puesto que su desempeño se vio afectado en menor medida, tan solo 1 dato de diferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De entre todos los modelos entrenados, el que presento mejor desempeño corresponde a la red de convolución no corrupta, aun así la diferencia entre el modelo entrenado con datos correctos vs datos corruptos es de 0.02 puntos de presición lo cual resulta sorprendente tomando en cuenta que 12,5% de los datos de entrenamiento son erroneos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
